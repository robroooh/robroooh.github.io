<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Behavioral Cloning</title>
  <meta name="description" content="Behavioral Cloning">

  <link rel="stylesheet" href="/assets/main.css">
  <link rel="canonical" href="http://robroooh.github.io/2017/11/01/behavioral-cloning.html">
  <link rel="alternate" type="application/rss+xml" title="SDC cave" href="/feed.xml">
  
  
</head>


  <body>

    <header class="site-header" role="banner">

  <div class="wrapper">
    
    
    <a class="site-title" href="/">SDC cave</a>
  
    
      <nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger">
          
            
            
          
            
            
            <a class="page-link" href="/about/">About</a>
            
          
            
            
          
            
            
          
            
            
          
        </div>
      </nav>
    
  </div>
</header>


    <main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title" itemprop="name headline">Behavioral Cloning</h1>
    <p class="post-meta">
      <time datetime="2017-11-01T20:25:27+01:00" itemprop="datePublished">
        
        Nov 1, 2017
      </time>
      </p>
  </header>

  <div class="post-content" itemprop="articleBody">
    <h1 id="behavioral-cloning"><strong>Behavioral Cloning</strong></h1>

<iframe width="560" height="315" src="https://www.youtube.com/embed/Wtxhrw1ssmU" frameborder="0" allowfullscreen=""></iframe>

<h4 id="1-model-architecture">1. Model architecture</h4>

<p>My model is based on the <a href="http://images.nvidia.com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px.pdf">nvidia’s paper</a>
here. Using 4 convolutional layers with ELU activation to introduce non-linearity (<a href="https://github.com/robroooh/CarND-Behavioral-Cloning-P3/blob/master/behavioral_cloning.ipynb">code</a> line 178).
The data was normalized in the model using a Keras lambda layer (<a href="https://github.com/robroooh/CarND-Behavioral-Cloning-P3/blob/master/behavioral_cloning.ipynb">code</a> line 172).</p>

<h4 id="2-reducing-overfitting-in-the-model">2. Reducing overfitting in the model</h4>

<p>The model contains dropout layers in order to reduce overfitting (<a href="https://github.com/robroooh/CarND-Behavioral-Cloning-P3/blob/master/behavioral_cloning.ipynb">code</a> lines 194).</p>

<p>The dataset was splitted to 80/20 train/validate dataset, so that the model won’t overfitted (<a href="https://github.com/robroooh/CarND-Behavioral-Cloning-P3/blob/master/behavioral_cloning.ipynb">code</a> line 65)</p>

<p>The model was tested by running it through the simulator and ensuring that the vehicle could stay on the track.</p>

<h4 id="3-model-parameter-tuning">3. Model parameter tuning</h4>

<p>The model used an adam optimizer, so the learning rate was automatically tuned (<a href="https://github.com/robroooh/CarND-Behavioral-Cloning-P3/blob/master/behavioral_cloning.ipynb">code</a> line 210).</p>

<h4 id="4-appropriate-training-data">4. Appropriate training data</h4>

<p>Training data was chosen to keep the vehicle driving on the road. I used udacity’s provided data because it was challenging
to make use of existing data and augmented them to solve the task. 
The data is from normally driving, and it provides left/right camera images which
able us to teach the car to learn from mistakes.</p>

<p>The details on data augmentation can be seen in the next section.</p>

<h3 id="model-architecture-and-training-strategy">Model Architecture and Training Strategy</h3>

<h4 id="1-solution-design-approach">1. Solution Design Approach</h4>

<p>The strategy used for deriving an architecture was to make the network that could
predict the angle solely from an image.</p>

<p>I did not start building the architecture from scratch. In fact, I used a cnn the exact same network as
in the <a href="http://images.nvidia.com/content/tegra/automotive/images/2016/solutions/pdf/end-to-end-dl-using-px.pdf">nvidia’s paper</a>.</p>

<p>The main reasons I thought this model was appropriate are that:</p>
<ol>
  <li>It combined many convolutional layers with appropriate sizes for feature engineering.</li>
  <li>It was proved by visualization of the activation of feature maps in the paper that the architecture is 
able to learn useful features on its own.</li>
</ol>

<p>To test how cool my model was, I splitted the dataset (image, steering_angle) into training/validation set.</p>

<p>The loss of my model was too high, that i decided to work more on data augmentation &amp; tuning left/right camera offset.</p>

<p>Moreover, I add <a href="https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf">dropout layers to prevent complex co-adaptation</a> &amp; overfitting 
between each of last 3 Fully Connected layers</p>

<p>The final step was to run the simulator to see how well the car was driving around track one. At first, the car always went off the track.
I notice this bahavior as it does not know how to get back to the center of lane.</p>

<p>To improve the driving behavior in these cases, I fine-tuned the left/right camera offset.
I found that 0.2 was an acceptable number which I could let the car drive for hours without going off track despite some jittery.</p>

<h4 id="2-final-model-architecture">2. Final Model Architecture</h4>

<p>Here is a (long) visualization of the architecture</p>

<p><img src="/assets/model_architecture.png" alt="alternate" title="Model Visualization" /></p>

<p>My model can be describe in details by the following list (on model.py lines xx-xx) :</p>

<ol>
  <li>Lambda layer - for normalization</li>
  <li>Cropping Layer - crop 50px from the top, 20px from the bottom</li>
  <li>24@5x5 stride-2 convolutional layer following by ELU Activation</li>
  <li>36@5x5 stride-2 convolutional layer following by ELU Activation</li>
  <li>48@5x5 stride-2 convolutional layer following by ELU Activation</li>
  <li>64@3x3 stride-1 convolutional layer following by ELU Activation</li>
  <li>Flatten layer</li>
  <li>1000 Fully Connected layer following by ELU Activation</li>
  <li>Dropout with p=0.5</li>
  <li>100 Fully Connected layer following by ELU Activation</li>
  <li>Dropout with p=0.5</li>
  <li>50 Fully Connected layer following by ELU Activation</li>
  <li>Dropout with p=0.5</li>
  <li>1 Fully Connected layer</li>
</ol>

<h4 id="3-creation-of-the-training-set--training-process">3. Creation of the Training Set &amp; Training Process</h4>

<p>To create a training set, I am more interested in using
udacity’s provided dataset than collecting my own dataset 
because of the following reasons</p>
<ul>
  <li>the dataset that udacity provides are small, left-biased, and 0-angles biased.
 Thus, it can’t be used directly.
 Trying to make it work involves much effort on understanding the data, task, 
 and architecture which I found challenging.</li>
  <li>the udacity’s task state that to recover the car from off-center to center can be
 done by collecting data which a car drives from the corner to the center, but I like
 how nvidia works on the offset adjustment for left/right camera images which is perfect
 for me to make the most out of existing data.</li>
</ul>

<p>To get myself into udacity’s data, I first explore the data</p>

<p><img src="/assets/data_exploration1.png" alt="alt text" title="Data Exploration: Before Augmentation" />
<em>The histrogram of the udacity’s dataset: the X-axis is the steering_angle 
and the Y-axis is the number that element on X-axis occurs (plot with bin=50)</em></p>

<blockquote>
  <p>I noticed some biased on the 0 angle, so I know that I need to balance the data.
Meanwhile I perceived the numbers of data for other angles are too small.
Data Augmentation is definitely needed here</p>
</blockquote>

<p><img src="/assets/data_augment1.png" alt="alt text" title="Data Augmentation" /></p>

<p>My Augmentation set here is:</p>
<ol>
  <li>Offset images from left/center/right camera
    <ul>
      <li>additional off-center shifts of images are then used 
for network to learn how to recover from mistakes or it will 
slowly drive off the center, the magnitude of 
shift value seem to be 0.2. I tried(0.08, 0.1, 0.2) the last one was the best,
but I’m sure it can be better</li>
    </ul>
  </li>
  <li>Resize the images to (128,128)
    <ul>
      <li>the numbers are from eyeballing. I tried 64,64 but I think there must 
be loss of information, so I try larger image sizes that my eyes &amp; 
training speed feel comfortable with</li>
    </ul>
  </li>
  <li>Flip
    <ul>
      <li>both the image along the horizontal-axis and angle</li>
    </ul>
  </li>
  <li>Random brightness <strong>or</strong> shadow on both flipped and non-flipped images
    <ul>
      <li>brightness adjustment
        <ul>
          <li>by adjusting gain in the range of 0.3-0.6</li>
        </ul>
      </li>
      <li>shadows shade
        <ul>
          <li>random vertical line with the width of 40 pixels</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>Cropping
    <ul>
      <li>the top for 50 pixels</li>
      <li>the bottom for 20 pixels</li>
    </ul>
  </li>
  <li>Randomly select only 10% of 0.0 angles dataset available</li>
</ol>

<p>Here is the result of after implementing data augmentation pipeline, there are much more 
data available to train from ~8k to ~50k</p>

<p><img src="/assets/data_exploration2.png" alt="alt text" title="Data Exploration: After Augmentation" /></p>

<p>However, the data is too big to fit in the memory, so I use Keras’s generator() to
do data real-time augmentation while training the model.</p>

<p>I finally randomly shuffled the data set and put 20% of the data into a validation set.
The validation set contains only image from center camera and no augmentation was done.</p>

<p>I used this training data for training the model. 
The validation set helped determine if the model was over or under fitting. 
The ideal number of epochs was 60 as evidenced by it seems that another 10 epochs
might make the network overfitted.</p>

<p><img src="/assets/train_valid.png" alt="alt text" title="Train/Validate loss" /></p>

<p>I used an adam optimizer, so no learning rate was adjusted.</p>

  </div>

  
</article>

      </div>
    </main>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">SDC cave</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li>
            
              SDC cave
            
            </li>
            
            <li><a href="mailto:"></a></li>
            
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/robroooh"><span class="icon icon--github"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">robroooh</span></a>

          </li>
          

          
          <li>
            <a href="https://twitter.com/"><span class="icon icon--twitter"><svg viewBox="0 0 16 16" width="16px" height="16px"><path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/></svg>
</span><span class="username"></span></a>

          </li>
          
        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p>A Udacity&#39;s self-driving car nanodegree student who love machine learning.</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
